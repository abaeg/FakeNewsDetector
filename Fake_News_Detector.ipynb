{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KS6Ce3LthhVw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlMw92o-iqmm"
   },
   "source": [
    "# **Pre-Processing**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SofuAfSiglxp"
   },
   "source": [
    "These cells clean text files and generate csv's for Test and Train data.Don't run these if you already have csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "nf3F4yP0haRY"
   },
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "!pip install Urduhack[tf] \n",
    "import urduhack \n",
    "urduhack.download()\n",
    "import re\n",
    "from urduhack.preprocessing import normalize_whitespace\n",
    "from urduhack.preprocessing import remove_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "dcu-XbXxhufB"
   },
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "  text = remove_punctuation(data)\n",
    "  text=re.sub(r\"[A-Za-z0-9@;:]\", \"\", text, flags=re.UNICODE)\n",
    "  text=text.replace(\"\\n \", \".\")\n",
    "  text=text.replace(\"\\n  \", \".\")\n",
    "  text=text.replace(\"\\n\", \".\")\n",
    "  text = re.sub(r'\\.+', \"\\n\", text)\n",
    "  text = normalize_whitespace(text)\n",
    "  text = remove_punctuation(text)\n",
    "  text=text.replace(\"\\n\", \" \")\n",
    "  text = normalize_whitespace(text)\n",
    "  return text\n",
    "\n",
    "Test_Fake=[]\n",
    "Test_Real=[]\n",
    "Train_Fake=[]\n",
    "Train_Real=[]\n",
    "for i in range (1,351):\n",
    "  with open('/content/drive/MyDrive/NLP5_Data/Train/Real/R ('+str(i)+').txt','r') as f:\n",
    "    text = f.read().replace('\\n', '')\n",
    "    Train_Real.append(clean(text)) \n",
    "for i in range (1,289):\n",
    "  with open('/content/drive/MyDrive/NLP5_Data/Train/Fake/F ('+str(i)+').txt','r') as f:\n",
    "    text = f.read().replace('\\n', '')\n",
    "    Train_Fake.append(clean(text))\n",
    "Train = Train_Fake+Train_Real\n",
    "L1 = [0] * len(Train_Fake)\n",
    "L2 = [1] * len(Train_Real)\n",
    "Class=L1+L2\n",
    "di = {'Text':Train,'Class':Class}\n",
    "df = pd.DataFrame(di)\n",
    "df.to_csv(\"/content/drive/MyDrive/NLP5_Data/Train.csv\",index=False)\n",
    "\n",
    "for i in range (1,151):\n",
    "  with open('/content/drive/MyDrive/NLP5_Data/Test/Real/R ('+str(i)+').txt','r') as f:\n",
    "    text = f.read().replace('\\n', '')\n",
    "    Test_Real.append(clean(text))\n",
    "for i in range (1,113):\n",
    "  with open('/content/drive/MyDrive/NLP5_Data/Test/Fake/F ('+str(i)+').txt','r') as f:\n",
    "    text = f.read().replace('\\n', '')\n",
    "    Test_Fake.append(clean(text))\n",
    "Test = Test_Fake + Test_Real\n",
    "L1 = [0] * len(Test_Fake)\n",
    "L2 = [1] * len(Test_Real)\n",
    "Class=L1+L2\n",
    "di = {'Text':Test,'Class':Class}\n",
    "df = pd.DataFrame(di)\n",
    "df.to_csv(\"/content/drive/MyDrive/NLP5_Data/Test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPBMHW7Zjatf"
   },
   "source": [
    "# **Loading CSV's**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThuliWc6hnEB"
   },
   "source": [
    "Loading csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "m2Rkonr1Oduu"
   },
   "outputs": [],
   "source": [
    "df_Train=pd.read_csv('/content/drive/MyDrive/NLP5_Data/Train.csv')\n",
    "df_Test=pd.read_csv('/content/drive/MyDrive/NLP5_Data/Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "PeSReYAXpaOf"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/NLP5_Data/stopwords-ur.txt','r') as f:\n",
    "  stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "Bz-MuXseJdQV"
   },
   "outputs": [],
   "source": [
    "Train = df_Train[\"Text\"].to_numpy()\n",
    "Class_Train = df_Train[\"Class\"].to_numpy()\n",
    "Test = df_Test[\"Text\"].to_numpy()\n",
    "Class_Test= df_Test[\"Class\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wK2lIUnrjTPg"
   },
   "source": [
    "# **Training**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "wrWRg8OyEVYy"
   },
   "outputs": [],
   "source": [
    "def RemoveStopwords(Text):\n",
    "  StopwordText=[]\n",
    "  for sentence in Text:\n",
    "    StopwordText.append( \" \".join( [wrd for wrd in sentence.split() if wrd not in stopwords]) )\n",
    "  return np.array(StopwordText)\n",
    "\n",
    "def TrainMultinomialNB(Text,Class):\n",
    "  V=Counter(( \" \".join(Text) ).split())\n",
    "  N=len(Text)\n",
    "  C=np.unique(Class)\n",
    "  prior={}\n",
    "  condprob={}\n",
    "  for c in C: \n",
    "    mask = Class==c\n",
    "    doc = Text[mask]  #Takes all texts of class c \n",
    "    Nc=len(doc)\n",
    "    prior[c]=Nc/N   #  priors{c1:P(c1), c2:P(c2) }  stores classes and their priors in dictionary\n",
    "    counts=Counter(wrd for txt in doc for wrd in txt.split()) #calculates counts of each token in doc\n",
    "    Nw = sum(counts.values())#number of texts in Class c \n",
    "    for w in V:\n",
    "      Ni=counts[w]  # count of word w in documents \n",
    "      p=(Ni+1)/(Nw + len(V))\n",
    "      condprob.setdefault(c,{})[w] = p  # condprob{ c1:{w1:P(w1|c1)}, c1:{w2:P(w2|c1)}...  }  stores class, word and conditional prob in dictionary\n",
    "    \n",
    "\n",
    "  return V, prior, condprob\n",
    "\n",
    "def TrainMultinomialNB_Boolean(Text,Class):\n",
    "  V=Counter(( \" \".join(Text) ).split())\n",
    "  N=len(Text)\n",
    "  C=np.unique(Class)\n",
    "  prior={}\n",
    "  condprob={}\n",
    "  for c in C:\n",
    "    mask = Class==c\n",
    "    TXTc = Text[mask]  #Takes all texts of class c \n",
    "    Nc=len(TXTc)\n",
    "    prior[c]=Nc/N   #  priors{c1:P(c1), c2:P(c2) }  stores classes and their priors in dictionary\n",
    "    doc=[  \" \".join(Counter(txt.split()).keys())  for txt in TXTc  ]  # Remove duplicates  TXTc \n",
    "    counts=Counter(wrd for txt in doc for wrd in txt.split()) #calculates counts of each token in doc\n",
    "    Nw = sum(counts.values())#number of texts in Class c\n",
    "    for w in V:\n",
    "      Ni=counts[w]  # count of word w in documents\n",
    "      p=(Ni+1)/(Nw + len(V))\n",
    "      condprob.setdefault(c, {})[w] = p  # condprob{ c1:{w1:P(w1|c1)}, c1:{w2:P(w2|c1)}...  }  stores class, word and conditional prob in dictionary\n",
    "  return V, prior, condprob\n",
    "  \n",
    "\n",
    "def ApplyMultinomialNB(C, V, prior, condprob, t,isBool):\n",
    "  if isBool:\n",
    "    t=[  \" \".join(Counter(txt.split()).keys())  for txt in [t]  ]  # Remove duplicates from test set\n",
    "  tokens=( \" \".join(t) ).split() #covert list to  string, split on space\n",
    "  W= [w for w in tokens if w in V ] \n",
    "  score={}\n",
    "  for  c in C:\n",
    "    # print('hereC',c ,end=' , ' )\n",
    "    score[c] = math.log(prior[c])\n",
    "    for  w in W:\n",
    "      score[c]+= math.log(condprob[c][w])  \n",
    "  \n",
    "  return max(score,key=score.get)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DevTt45kGgdz"
   },
   "source": [
    "# **Testing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "d4QxGduBhjTD"
   },
   "outputs": [],
   "source": [
    "# Train= np.array(['chineese beijin chineese','chineese shanghai chineese','chineese food','tokyo japan'] )\n",
    "# Class_Train= np.array([0,0,0,1])\n",
    "# Test= np.array(['chineese chineese chineese tokyo','chineese shanghai chineese'] )\n",
    "# Class_Test= np.array([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GrCJi9SGG3Yj",
    "outputId": "64391488-be6a-4d0e-a210-be08e29f254a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB  [1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "NB_S  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "NBB  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "NBB_S  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "C=[0,1]  #unique set of all classes\n",
    "Test_S = RemoveStopwords(Test)\n",
    "Train_S = RemoveStopwords(Train)\n",
    "\n",
    "#- Naive Bayes without stopwords\n",
    "V , prior, condprob = TrainMultinomialNB(Train,Class_Train)\n",
    "NB=[ ApplyMultinomialNB(C, V, prior, condprob, txt,False) for txt in Test]\n",
    "print(\"NB \",NB)\n",
    "\n",
    "#- Naive Bayes with stopwords\n",
    "V , prior, condprob = TrainMultinomialNB(Train_S ,Class_Train)\n",
    "NB_S=[ ApplyMultinomialNB(C, V, prior, condprob, txt,False) for txt in Test_S]\n",
    "print(\"NB_S \",NB_S)\n",
    "\n",
    "#- Boolean Naive Bayes without stopwords\n",
    "V, prior, condprob = TrainMultinomialNB_Boolean(Train ,Class_Train)\n",
    "NBB=[ ApplyMultinomialNB(C, V, prior, condprob, txt,True) for txt in Test]\n",
    "print(\"NBB \",NBB)\n",
    "\n",
    "#--Boolean Naive Bayes with stopwords\n",
    "V , prior, condprob = TrainMultinomialNB_Boolean(Train_S ,Class_Train)\n",
    "NBB_S=[ ApplyMultinomialNB(C, V, prior, condprob, txt,True) for txt in Test_S]\n",
    "print(\"NBB_S \",NBB_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDaCLKzCj1yx"
   },
   "source": [
    "# **..................................................... REPORT ......................................................**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrmdKxPIj1y1"
   },
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "X_nW8ivQQueX"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U_DBSeM3SmXP",
    "outputId": "1f0f4b12-2ceb-49d4-83b7-fc066e97f341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score\n",
      "   NB without stopwords : 0.5763358778625954 \n",
      "   NB with stopwords : 0.5725190839694656\n",
      "   NB Boolean without stopwords : 0.7633587786259542 \n",
      "   NB Boolean with stopwords : 0.7633587786259542\n",
      "Precision score\n",
      "   NB without stopwords : 0.5859030837004405 \n",
      "   NB with stopwords : 0.5725190839694656\n",
      "    NB Boolean without stopwords : 0.7619047619047619 \n",
      "   NB Boolean with stopwords : 0.7650602409638554\n",
      "Recall score\n",
      "   NB without stopwords : 0.8866666666666667 \n",
      "   NB with stopwords : 1.0\n",
      "   NB Boolean without stopwords : 0.8533333333333334 \n",
      "   NB Boolean with stopwords : 0.8466666666666667\n",
      "F1 score\n",
      "   NB without stopwords : 0.7055702917771883 \n",
      "   NB with stopwords : 0.7281553398058253\n",
      "   NB Boolean without stopwords : 0.8050314465408804 \n",
      "   NB Boolean with stopwords : 0.8037974683544303\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score\\n   NB without stopwords :', accuracy_score(Class_Test,NB),'\\n   NB with stopwords :', accuracy_score(Class_Test,NB_S))\n",
    "print('   NB Boolean without stopwords :', accuracy_score(Class_Test,NBB),'\\n   NB Boolean with stopwords :', accuracy_score(Class_Test,NBB_S))\n",
    "\n",
    "print('Precision score\\n   NB without stopwords :', precision_score(Class_Test,   NB),'\\n   NB with stopwords :', precision_score(Class_Test,NB_S))\n",
    "print('    NB Boolean without stopwords :', precision_score(Class_Test,NBB),'\\n   NB Boolean with stopwords :', precision_score(Class_Test,NBB_S))\n",
    "\n",
    "print('Recall score\\n   NB without stopwords :', recall_score(Class_Test,   NB),'\\n   NB with stopwords :', recall_score(Class_Test,NB_S))\n",
    "print('   NB Boolean without stopwords :', recall_score(Class_Test,NBB),'\\n   NB Boolean with stopwords :', recall_score(Class_Test,NBB_S))\n",
    "\n",
    "print('F1 score\\n   NB without stopwords :', f1_score(Class_Test,NB),'\\n   NB with stopwords :', f1_score(Class_Test,   NB_S))\n",
    "print('   NB Boolean without stopwords :', f1_score(Class_Test,NBB),'\\n   NB Boolean with stopwords :', f1_score(Class_Test,NBB_S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZogpjYUomMI"
   },
   "source": [
    "It is evident from the accuarcies above that removal of stopwords brings a veriy significant improvement in the results.And removing duplicates i.e Boolean NB gives best results of the four methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLscCnyMolH0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "QlMw92o-iqmm",
    "VPBMHW7Zjatf"
   ],
   "name": "i17_0327.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
